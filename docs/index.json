[{"content":"In the realm of technology\u0026rsquo;s grand domain,\nDeuts Log stands proud, its knowledge to proclaim,\nAn eclectic assortment, a journey to embark,\nThrough subjects diverse, a technological spark.\nFrom deep within its digital core,\nIt shares insights and more, galore,\nA tapestry woven with threads so fine,\nUnraveling mysteries in every line.\nOnce upon Tumblr\u0026rsquo;s vibrant stage,\nDeuts Log found its early page,\nBut time\u0026rsquo;s tide shifted, winds did blow,\nAnd changes came, as we all know.\nAutomattic\u0026rsquo;s grasp, a hopeful chance,\nTo infuse new life, make Tumblr dance,\nYet, alas, the vigor seemed to fade,\nLeaving Deuts Log in a contemplative shade.\nUnmet expectations, hopes held high,\nTransformation\u0026rsquo;s wings failed to fly,\nIn the unprogressive state it stayed,\nAs Tumblr\u0026rsquo;s spirit seemed to slowly fade.\nBut a reboot beckoned, a fresh new morn,\nA decision made, from the old to be reborn,\nNo transfer of tales, no digital freight,\nInstead, a choice to recreate.\nA Hugo-based haven, a virtual space,\nWhere Deuts Log finds a new embrace,\nFrom modest past traffic to a future untold,\nA canvas reset, a story to unfold.\nSo here\u0026rsquo;s to beginnings, to paths anew,\nTo Deuts Log\u0026rsquo;s revival, bold and true,\nA tale of technology, of change and choice,\nIn poetic verses, we give it voice.\n","permalink":"https://deuts.org/a/","summary":"\u003cp\u003eIn the realm of technology\u0026rsquo;s grand domain,\u003cbr\u003e\n\u003cstrong\u003eDeuts Log\u003c/strong\u003e stands proud, its knowledge to proclaim,\u003cbr\u003e\nAn eclectic assortment, a journey to embark,\u003cbr\u003e\nThrough subjects diverse, a technological spark.\u003c/p\u003e\n\u003cp\u003eFrom deep within its digital core,\u003cbr\u003e\nIt shares insights and more, galore,\u003cbr\u003e\nA tapestry woven with threads so fine,\u003cbr\u003e\nUnraveling mysteries in every line.\u003c/p\u003e\n\u003cp\u003eOnce upon \u003ca href=\"https://deuts.tumblr.com\"\u003eTumblr\u003c/a\u003e\u0026rsquo;s vibrant stage,\u003cbr\u003e\nDeuts Log found its early page,\u003cbr\u003e\nBut time\u0026rsquo;s tide shifted, winds did blow,\u003cbr\u003e\nAnd changes came, as we all know.\u003c/p\u003e","title":"About"},{"content":" Name Email Address Message ","permalink":"https://deuts.org/c/","summary":"\u003cform id=\"fs-frm\" name=\"simple-contact-form\" accept-charset=\"utf-8\" action=\"https://formspree.io/f/xleyjlpp\" method=\"post\"\u003e\n  \u003cfieldset id=\"fs-frm-inputs\"\u003e\n    \u003clabel for=\"full-name\"\u003eName\u003c/label\u003e\n    \u003cinput type=\"text\" name=\"name\" id=\"name\" placeholder=\"Full Name\" required=\"\"\u003e\n    \u003clabel for=\"email-address\"\u003eEmail Address\u003c/label\u003e\n    \u003cinput type=\"email\" name=\"_replyto\" id=\"email-address\" placeholder=\"name@email.com\" required=\"\"\u003e\n    \u003clabel for=\"message\"\u003eMessage\u003c/label\u003e\n    \u003ctextarea rows=\"5\" name=\"message\" id=\"message\" placeholder=\"Describe your concerns here.\" required=\"\"\u003e\u003c/textarea\u003e\n    \u003cinput type=\"hidden\" name=\"_subject\" id=\"email-subject\" value=\"Contact Form Submission\"\u003e\n  \u003c/fieldset\u003e\n  \u003cinput type=\"submit\" value=\"Send\"\u003e\n  \u003ch3 id=\"my-form-status\"\u003e\u003c/h3\u003e\n\u003c/form\u003e\n\u003cscript\u003e\n  var form = document.getElementById(\"fs-frm\");\n  \n  async function handleSubmit(event) {\n    event.preventDefault();\n    var status = document.getElementById(\"my-form-status\");\n    var data = new FormData(event.target);\n    fetch(event.target.action, {\n      method: form.method,\n      body: data,\n      headers: {\n          'Accept': 'application/json'\n      }\n    }).then(response =\u003e {\n      if (response.ok) {\n        status.innerHTML = \"Your feedback has been received. Thank you!\";\n        form.reset()\n      } else {\n        response.json().then(data =\u003e {\n          if (Object.hasOwn(data, 'errors')) {\n            status.innerHTML = data[\"errors\"].map(error =\u003e error[\"message\"]).join(\", \")\n          } else {\n            status.innerHTML = \"Oops! There was a problem submitting your form\"\n          }\n        })\n      }\n    }).catch(error =\u003e {\n      status.innerHTML = \"Oops! There was a problem submitting your form\"\n    });\n  }\n  form.addEventListener(\"submit\", handleSubmit)\n\u003c/script\u003e\n\n\u003cstyle\u003e \n#fs-frm input,\n#fs-frm select,\n#fs-frm textarea,\n#fs-frm fieldset,\n#fs-frm optgroup,\n#fs-frm label,\n#fs-frm #card-element:disabled {\n  font-family: inherit;\n  font-size: 100%;\n  color: inherit;\n  border: none;\n  border-radius: 0;\n  display: block;\n  width: 100%;\n  padding: 0;\n  margin: 0;\n  -webkit-appearance: none;\n  -moz-appearance: none;\n}\n#fs-frm label,\n#fs-frm legend,\n#fs-frm ::placeholder {\n  font-size: .825rem;\n  margin-bottom: .5rem;\n  padding-top: .2rem;\n  display: flex;\n  align-items: baseline;\n}\n\n \n#fs-frm input,\n#fs-frm select,\n#fs-frm textarea,\n#fs-frm #card-element {\n  border: 1px solid rgba(0,0,0,0.2);\n  background-color: rgba(65, 65, 65, 0.9);\n  padding: .75em 1rem;\n  margin-bottom: 1.5rem;\n}\n#fs-frm input:focus,\n#fs-frm select:focus,\n#fs-frm textarea:focus {\n  background-color: #666;\n  outline-style: solid;\n  outline-width: thin;\n  outline-color: gray;\n  outline-offset: -1px;\n}\n#fs-frm [type=\"text\"],\n#fs-frm [type=\"email\"] {\n  width: 100%;\n}\n#fs-frm [type=\"button\"],\n#fs-frm [type=\"submit\"],\n#fs-frm [type=\"reset\"] {\n  width: auto;\n  cursor: pointer;\n  -webkit-appearance: button;\n  -moz-appearance: button;\n  appearance: button;\n}\n#fs-frm [type=\"button\"]:focus,\n#fs-frm [type=\"submit\"]:focus,\n#fs-frm [type=\"reset\"]:focus {\n  outline: none;\n}\n#fs-frm [type=\"submit\"],\n#fs-frm [type=\"reset\"] {\n  margin-bottom: 0;\n}\n\n#fs-frm [type=\"submit\"] {\n  background-color: #555;\n}\n\n#fs-frm [type=\"submit\"]:hover {\n  background-color: #999;\n}\n\n#fs-frm select {\n  text-transform: none;\n}\n\n#fs-frm [type=\"checkbox\"] {\n  -webkit-appearance: checkbox;\n  -moz-appearance: checkbox;\n  appearance: checkbox;\n  display: inline-block;\n  width: auto;\n  margin: 0 .5em 0 0 !important;\n}\n\n#fs-frm [type=\"radio\"] {\n  -webkit-appearance: radio;\n  -moz-appearance: radio;\n  appearance: radio;\n}\n\n \n#fs-frm fieldset.locale input[name=\"city\"],\n#fs-frm fieldset.locale select[name=\"state\"],\n#fs-frm fieldset.locale input[name=\"postal-code\"] {\n  display: inline;\n}\n#fs-frm fieldset.locale input[name=\"city\"] {\n  width: 52%;\n}\n#fs-frm fieldset.locale select[name=\"state\"],\n#fs-frm fieldset.locale input[name=\"postal-code\"] {\n  width: 20%;\n}\n#fs-frm fieldset.locale input[name=\"city\"],\n#fs-frm fieldset.locale select[name=\"state\"] {\n  margin-right: 3%;\n}\n\u003c/style\u003e","title":"Contact"},{"content":"Just when I thought I was done blogging about bad services, here I am again. I couldn\u0026rsquo;t resist letting this one out.\nIt\u0026rsquo;s been almost 3 days without internet at home in Salcedo Village, Makati. The provider? None other than the reliable PLDT.\nTheir initial response when I reported the issue? I was assigned a ticket stating restoration may take—wait for it—up to 36 hours. Not great, but manageable, right? Fast forward 72 hours (double the time promised), and what do I get? A message saying they\u0026rsquo;ll provide an update (not even a fix!) within another 48 hours.\nPLDT, it seems, has no sense of urgency to address the concerns of its paying customers. If you\u0026rsquo;re in the internet provider business, 1 day of downtime might be acceptable, but three days? That\u0026rsquo;s absurd.\nNow, thanks to PLDT’s stellar service, I’m forced to use my mobile hotspot for the weekend. What a bummer. I’ll be eating into my phone’s data provision just to get work done. Honestly, PLDT should be the one footing the bill for my backup internet.\nAnd let’s not forget their audacity to lock customers into 3-year contracts! I wish Converge had been available in my building when I was shopping for wired internet 10 months ago. At least I\u0026rsquo;d have a shot at better service.\nSo, PLDT service is actually this bad, huh? I always thought those horror stories were just urban myths—tales spun to scare unsuspecting subscribers. But nope, they’re real. And here I am, starring in my own PLDT nightmare.\nUpdate 11/17/2024 6:13am After 85 hours of downtime, my PLDT internet was finally back online.\n","permalink":"https://deuts.org/p/pldt-no-internet-downtime/","summary":"After almost 3 days without internet in Salcedo Village, Makati, PLDT\u0026rsquo;s service woes are proving to be more than just urban legends. From promises of a 36-hour restoration to endless delays, this is my frustrating experience with their \u0026ldquo;reliable\u0026rdquo; service.","title":"The PLDT Horror: 72 Hours (and Counting) Without Internet"},{"content":"Securing your web services with SSL/TLS certificates is crucial, especially when dealing with public servers. Using Caddy’s built-in HTTPS functionality is easy, but situations involving firewalls, CGNAT, or a lack of access to port 80/443 benefit from Cloudflare\u0026rsquo;s DNS challenge for seamless certificate automation. This tutorial outlines how to build a custom Caddy Docker image that integrates Cloudflare’s DNS module using xcaddy to streamline this process.\nWhy Opt for Cloudflare DNS Challenge? Caddy\u0026rsquo;s HTTP and TLS challenges work well for most, but the DNS challenge shines when:\nYour server is behind a firewall or CGNAT. You want to avoid exposing ports 80 and 443 to the public. Your setup includes a load balancer or other restrictive networking configurations. This method authenticates your domain ownership via the Cloudflare DNS API, allowing Caddy to fetch certificates without the need for open HTTP/HTTPS ports. See How the DNS Challenge Works for more info.\nBuilding a Custom Caddy Image The Dockerfile follows a multi-stage build:\nBuilder Stage: We use xcaddy to compile Caddy with the Cloudflare DNS provider plugin. Final Stage: The resulting binary is copied to a fresh Caddy image, creating a production-ready and lean container. How to Set Up and Run Step 1: Create a Custom Docker Network (Optional) Creating a custom Docker network keeps services isolated and minimizes the need to expose ports on your VPS:\ndocker network create caddynetwork If you choose a different network name, ensure the compose.yml file reflects it.\nStep 2: Create your compose.yml file Below is the compose.yml configuration, which builds a custom Caddy Docker image and sets up networking for enhanced security:\nservices: caddy: build: context: . dockerfile: Dockerfile container_name: caddy environment: - CLOUDFLARE_API_TOKEN=\u0026lt;ENTER YOUR TOKEN HERE\u0026gt; - CADDY_ACME_EMAIL=\u0026lt;ENTER YOUR EMAIL HERE\u0026gt; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./Caddyfile:/etc/caddy/Caddyfile - ./data:/data - ./config:/config restart: unless-stopped networks: - caddynetwork networks: caddynetwork: external: true Step 3: Configure compose.yml Rename _compose.yml to compose.yml and set your environment variables:\nenvironment: - CLOUDFLARE_API_TOKEN=\u0026lt;YOUR_TOKEN\u0026gt; - CADDY_ACME_EMAIL=\u0026lt;YOUR_EMAIL\u0026gt; CLOUDFLARE_API_TOKEN: Ensure this token has permissions for DNS zone edits on Cloudflare. CADDY_ACME_EMAIL: This email will be used by ACME for certificate registration. Step 4: Build and Run Use Docker Compose to build and start the container in detached mode:\ndocker compose up -d --build Step 5: Edit Your Caddyfile Adjust your Caddyfile with the necessary domain and service details. Here’s a sample:\nexample.com { reverse_proxy localhost:8080 tls { dns cloudflare {env.CLOUDFLARE_API_TOKEN} } } Replace example.com with your actual domain. Set localhost to the name or IP of your backend service. Adjust 8080 to the port where your application runs. With this setup, Caddy will use Cloudflare’s DNS challenge to obtain certificates, keeping your services secure without exposing common HTTP/HTTPS ports.\nHow the DNS Challenge Works Typically, to issue an SSL/TLS certificate, Caddy (or any Certificate Authority, CA) needs to verify that you actually own the domain in question. Normally, this is done by:\nHTTP Challenge: Verifying domain ownership by placing a file on an accessible web server running on port 80. TLS-ALPN Challenge: Verifying via a specific response from a server running on port 443. However, both these methods require public access to specific ports on the server (80 or 443), which may not be feasible if:\nYour server is behind a firewall or behind Carrier-Grade NAT (CGNAT) on a network that doesn’t expose external ports. You don’t want to expose ports 80 and 443 publicly for security or regulatory reasons. You’re using a load balancer or reverse proxy setup where ports may not map directly to a specific server. Why the Cloudflare DNS Challenge Solves This The DNS challenge verifies domain ownership by checking for specific DNS records instead of requiring public access to your server. Here’s how it works:\nCaddy uses Cloudflare’s API to add a special TXT record to your domain’s DNS settings (hosted on Cloudflare). The CA (e.g., Let’s Encrypt) queries Cloudflare’s DNS servers for the TXT record to verify domain ownership. Once verified, the CA issues a certificate to Caddy for your domain, which it saves and manages. Because the DNS challenge doesn’t involve your server’s IP address or port accessibility, it allows Caddy to obtain certificates even when direct access to your server is restricted.\nWhy You Need Cloudflare API Permissions The DNS challenge requires dynamically creating and removing DNS records on your domain during each certificate renewal. By providing Caddy with Cloudflare’s API token, you enable it to manage the necessary DNS records for verification, fully automating the certificate issuance and renewal process without exposing any ports.\nIn summary, the Cloudflare DNS challenge is essential when public port access is unavailable or undesirable. It’s a robust way to automate certificate management, especially in complex network environments, by verifying domain ownership through DNS rather than HTTP or TLS ports.\n","permalink":"https://deuts.org/p/caddy-docker-cloudflare-dns-challenge/","summary":"This guide provides a custom Dockerized Caddy image setup that uses Cloudflare DNS for SSL/TLS verification, ideal for servers behind firewalls or NAT.","title":"Custom Caddy Docker image with Cloudflare DNS challenge support"},{"content":"HTTP Basic Authentication offers a quick way to secure access to internal or staging services in a Docker environment, especially for controlled scenarios where high security isn’t critical. Although credentials sent via Basic Authentication can be decoded if intercepted, combining it with HTTPS ensures encryption, making it suitable for simple use cases. Here, we’ll set up Caddy as a reverse proxy with Basic Authentication to limit access to a service running in another container.\nScenario Overview In this example, we have:\nA Caddy container named caddy A service container named sampledockercontainer, which is the service we’re protecting, running on port 8080 Both containers are on the same Docker network The goal is to limit access to the sampledockercontainer service with Basic Authentication Setting Up the Caddyfile Start with a simple Caddyfile that proxies traffic to the sampledockercontainer:\nexample.com { reverse_proxy sampledockercontainer:8080 } This configuration forwards all requests for example.com to the sampledockercontainer service within the Docker network.\nGenerating a Hashed Password for Authentication Since Caddy is running as a Docker container, use the following docker exec command to generate a hashed password for authentication:\ndocker exec -it caddy caddy hash-password --plaintext \u0026#39;yoursupersecretpassword\u0026#39; Assuming your username is user, the command returns a hashed password like:\n$2a$14$UnmpufOS3hIBsW5Jn.lbpe0qtoa5kCKYfJNPGsgnOr2D6mPma8aPm Adding Basic Authentication to the Caddyfile Update the Caddyfile to include Basic Authentication, securing access to example.com:\nexample.com { basicauth { user $2a$14$UnmpufOS3hIBsW5Jn.lbpe0qtoa5kCKYfJNPGsgnOr2D6mPma8aPm } reverse_proxy sampledockercontainer:8080 } Now, visitors will be prompted for a username and password before gaining access.\nRestricting Authentication to Specific Paths You may want only certain paths to require authentication, allowing general access to others. For example, you can limit Basic Authentication to the /admin path while keeping the root URL publicly accessible:\nexample.com { basicauth /admin/* { user $2a$14$UnmpufOS3hIBsW5Jn.lbpe0qtoa5kCKYfJNPGsgnOr2D6mPma8aPm } reverse_proxy /admin/* sampledockercontainer:8080 reverse_proxy /* sampledockercontainer:8080 } With this setup:\nRequests to example.com are open to all users. Requests to example.com/admin/ require a valid username and password. Conclusion This Caddyfile setup demonstrates a simple, effective way to protect containerized services with Basic Authentication. For Dockerized applications, Caddy makes it easy to limit access to sensitive areas, securing your internal or staging environments with minimal configuration.\n","permalink":"https://deuts.org/p/docker-caddy-basic-auth/","summary":"Learn how to secure a Docker-hosted service with Caddy’s HTTP Basic Authentication, an effective way to protect access in controlled environments. This guide walks through configuring Caddy as a reverse proxy with Basic Authentication to limit access to a containerized service.","title":"Securing Your Docker-Hosted Service with HTTP Basic Authentication in Caddy"},{"content":"Why use callouts Callouts in notes or web content are a secret weapon for enhancing readability and guiding the reader’s focus. By spotlighting key points such as tips, warnings, or action items, callouts break up dense text, making it visually engaging and easy to navigate—perfect for readers who like to skim for the good stuff. Think of them as visual pitstops for key information.\nBeyond making content look better, callouts help readers retain critical details. By emphasizing important points, callouts make it more likely that the message will stick long after the reader has moved on. Plus, they boost engagement, encouraging readers to linger on essential details they might otherwise miss. In short, callouts don’t just share information; they make it memorable.\nHugo support for callouts Hugo doesn’t come with built-in support for callouts, but there’s a simple solution—shortcodes! By using a shortcode, you can add custom callouts to your Hugo content with ease.\nHugo shortcode for callouts Thanks to Cosimameyer’s tutorial on implementing callouts in Hugo, we can use the following examples (with underlying shortcode syntax for download):\nInfo Information\rThis is an informational callout with default styling. View underlying shortcode.\nSuccess Success\rTell me more about your success story. View underlying shortcode.\nError Mistake\rYou're making a big mistake! View underlying shortcode.\nWarning Caution\rYou have been warned! View underlying shortcode.\nOthers Nothing\rThis is nothing! View underlying shortcode.\n","permalink":"https://deuts.org/p/hugo-callouts/","summary":"Learn how to use callouts to improve readability and focus in your notes or web content. This post covers the importance of callouts, Hugo\u0026rsquo;s support for them, and how to implement custom callouts with shortcodes.","title":"Enhancing Readability with Callouts in Hugo"},{"content":"Recent controversies surrounding Matt Mullenweg and WP Engine have led me to rethink my blogging platform. While WordPress has long been a popular CMS, I’m increasingly drawn to the simplicity and control of static site generators like Hugo.\nWhy Consider Hugo Over WordPress? Hugo’s performance and security benefits are difficult to ignore:\nPerformance: Hugo serves static HTML files without any backend load, delivering high-speed access. Security: Without a database or PHP reliance, Hugo sites inherently avoid many vulnerabilities that affect WordPress. Customization: Total control over templates and layouts eliminates the plugin bloat that can slow down WordPress. Version Control: Working with Git keeps my changes organized and easy to revert if needed. Low Maintenance: No frequent updates or patches—Hugo just works as-is, saving time on upkeep. Despite these advantages, I hesitated because managing content directly in Hugo, especially from the terminal, felt clunky compared to the WordPress dashboard. Using VS Code helped, but it was still missing that CMS simplicity.\nMy New Workflow: Obsidian and Hugo with Minimal Dependencies I\u0026rsquo;ve now found a balance that works without tying me to specific plugins that may become obsolete or unsupported. Here’s what my current workflow looks like:\nObsidian with Templater and Dataview Plugins:\nI use Obsidian for creating and editing posts, keeping my Hugo directory aligned with my Obsidian vault. With Templater, I can build posts from templates and generate shortcodes. The Dataview plugin helps me get an overview of post summaries without impacting the publishing process. Notably, I’ve chosen not to use plugins like Hugo Publish or Static Site MD Exporter. Relying on such plugins could cause future complications if they’re discontinued, so I’ve kept my setup as independent as possible. Building with Hugo:\nAfter editing posts, I use Hugo to generate HTML files, configured to output to a docs folder for simplicity. Publishing to GitHub Pages:\nPushing my Hugo directory, including the docs folder, to GitHub triggers publishing through GitHub Pages. This approach enables a completely self-managed site without relying on an external CMS. If I don’t have Obsidian access, I can still edit content through GitHub’s web interface, pull the changes, run Hugo to build, and push the updated site back. Direct Edit Links:\nFor convenience, each site page includes a subtle edit button linking to the GitHub page source, making updates simple. Considering Migration While I’m currently set on Hugo for future posts, the complexity involved in migrating my WordPress content — nearly 900 posts and 2,500 comments — makes me pause. Migrating everything isn’t an easy task, so for now, I’m planning to blog on Hugo going forward while exploring efficient ways to gradually bring over my legacy content.\nThis workflow with Hugo, Obsidian and Github Pages has struck the right balance, letting me blog on my terms without depending on plugins or external services. Obsidian, Hugo, and GitHub all rely on Markdown, creating a seamless experience where these tools work in perfect harmony. For anyone seeking a lean, self-reliant approach to publishing, this setup might be the perfect fit.\n","permalink":"https://deuts.org/p/build-hugo-obsidian-github-pages-workflow/","summary":"Recent WordPress controversies have pushed me toward Hugo for blogging, drawn by its performance, security, and simplicity. My workflow with Obsidian and Hugo minimizes dependencies, offering a streamlined, self-hosted solution.","title":"Building an Efficient Blogging Workflow with Hugo, Obsidian, and GitHub Pages"},{"content":"When starting a new project, it\u0026rsquo;s best practice to initialize the Git repository with main as the default branch (instead of the usual master). This post will walk you through how to set up your local repository, link it to a GitHub repository, and push your changes, including the setup for authentication using GitHub’s Personal Access Token (PAT).\n1. Initialize a Git Repository with main as the Default Branch Begin by navigating to the directory where your project is located. Run the following commands to initialize Git with the main branch directly.\ncd /path/to/your/project git init --initial-branch=main 2. Stage and Commit Your Files Next, add all files to the staging area and commit them:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; 3. Link Your Local Repository to GitHub Now, set up a connection to your GitHub repository. Instead of using the default origin label for the remote, you can specify a custom label like github, which will allow flexibility with multiple remotes if needed.\ngit remote add github https://github.com/deuts/deuts.github.io.git 4. Push to GitHub without Using origin For the first push, use the -u flag to set github as the default remote:\ngit pull github main --rebase git push -u github main After this initial push, you can use git push and git pull directly without specifying the remote every time, as Git will use github by default for this branch.\n5. Authentication: When You\u0026rsquo;ll Need Your Email and Personal Access Token (PAT) When pushing to GitHub for the first time, Git will prompt you to set up authentication. Here’s what to expect:\nEmail: If this is the first time using Git on your machine, Git will ask you to set your email and name:\ngit config --global user.email \u0026#34;you@example.com\u0026#34; git config --global user.name \u0026#34;Your Name\u0026#34; This is required for all commits, as GitHub associates your email with your GitHub account.\nPersonal Access Token (PAT): Instead of your GitHub password, Git now requires a PAT for authentication. When prompted to log in, enter your username and the PAT as the password.\nTo create a PAT:\nGo to GitHub’s PAT settings. Generate a token with repo access. Use this token in place of your password when Git requests authentication. Common Commands Summary Initialize a Git repository with main: git init --initial-branch=main Add and commit changes: git add . git commit -m \u0026#34;Commit message\u0026#34; Add a custom remote (github) and push: git remote add github https://github.com/deuts/deuts.github.io.git git push -u github main Push and pull with defaults: git push # no need for origin git pull # no need for origin With this setup, you\u0026rsquo;ll be using the main branch, avoiding the default origin label, and ensuring secure access with GitHub’s Personal Access Token (PAT) system.\nWatch this video if you want for more context \u0026nbsp; ","permalink":"https://deuts.org/p/git-repository-to-github/","summary":"A guide on initializing a Git repository in Linux with main as the default branch and securely pushing to GitHub using a Personal Access Token (PAT), ideal for those looking to streamline their Git setup.","title":"Setting Up a Git Repository for Pushing to GitHub"},{"content":"When combining Obsidian with Hugo for a streamlined blogging workflow, several plugins claim to offer an easy publishing process, but some fall short due to misalignment with Hugo’s expectations. In this post, I’ll highlight some common issues with the plugins I’ve encountered.\nThe Problem with Available Obsidian Plugins Hugo Publish One plugin that attempts to bridge the gap between Obsidian and Hugo is Hugo Publish. This plugin lets you assign tags to notes, which it then processes into Markdown files that can be used with Hugo. However, there\u0026rsquo;s a significant flaw: if you assign a blog tag to your notes for processing, the plugin carries this tag over to the output .md files. This means that every post, when built with Hugo, will include the blog tag, which is not only redundant but goes against the purpose of using tags as categories or topics.\nThe problem here is that Hugo Publish does not account for the need to handle tags dynamically for each post. In Hugo, tags are often used for grouping and categorization, and repeating the same tag across all posts diminishes their value.\nStatic Site MD Exporter Another plugin, Static Site MD Exporter, offers a different approach by publishing the processed .md files directly to GitHub. From there, you can use git pull to bring them into your Hugo project and build the site.\nThis plugin offers a useful feature: by adding published: true in the frontmatter of a note, it marks the note for processing and publishing. However, this is where things start to break down when using Hugo. The published property is actually an alias for publishDate in Hugo, which expects a date value, not a boolean. As a result, when you run the hugo build command, Hugo throws an error because it cannot interpret published: true correctly.\nSummary Both the Hugo Publish and Static Site MD Exporter plugins offer valuable functionality, but they fall short in handling tags and frontmatter properties according to Hugo’s conventions. The inability of these plugins to properly manage publishDate and tags leads to errors when building the Hugo site and creates unnecessary redundancy in post tags. These are important issues that need to be addressed in order for the workflow to function smoothly with Hugo.\n","permalink":"https://deuts.org/p/hugo-and-obsidian-workflow/","summary":"A guide on how to use Obsidian in conjunction with Hugo for a streamlined blogging workflow, highlighting the issues with existing Obsidian plugins and their impact on Hugo builds.","title":"Hugo and Obsidian workflow"},{"content":"I\u0026rsquo;m just leaving this here for my future reference. This is the faster way to generate running total in Power Query.\n\u0026nbsp; Though, I think this next video has more useful code process:\n\u0026nbsp; ","permalink":"https://deuts.org/p/list-generate-running-total/","summary":"You want a more efficient way of generating running total in Power Query?","title":"Using List.Generate to Generate Running Total Column in Power Query"},{"content":"There is a 2-year old thread on Reddit. Many in the comments say it doesn\u0026rsquo;t make sense.\nI\u0026rsquo;ve successfully implemented Samba natively in the past. The problem with native implementation is that you can take note of the procedures you\u0026rsquo;ve made to make it running, but when you need to reinstall your Linux OS or install in another box, you have to go through all those procedures again. With Docker, especially if you already have a working docker compose file, you just need to back up and/or carry over that yml file to the new box.\nSo here are the resources I will need to revisit in the future for a Docker implementation of Samba:\nCrazy Max - docker compose file ServerContainers Samba - sample docker compose file ","permalink":"https://deuts.org/p/samba-on-docker/","summary":"Many say Samba on Docker doesn\u0026rsquo;t make sense. But does it?","title":"Samba Docker or Native"},{"content":"Browsing through Jim\u0026rsquo;s Garage on Youtube, and his Github repository, I was inspired to finally give these Docker apps a try:\nImmich Paperless-ngx Vikunja rClone Restic ","permalink":"https://deuts.org/p/my-docker-apps-to-install-in-2024/","summary":"This is a checklist of Docker apps I want to spin up and test in the near future.","title":"Docker Apps to Install"},{"content":"In August, I posted about my problems running Vikunja via Docker.\n\u0026nbsp; Based on the above video, however, there seems to be no more requirement in a Docker install of Vikunja for a separate API container?\nIs this a sign I should start using Vikunja now? Let\u0026rsquo;s see. As soon as I get the time.\nThe Docker Compose File The docker-compose file below (grabbed from here) involves using Traefik as the reverse proxy, but I should be able to customize for my use case, as I\u0026rsquo;m using NPM.\nversion: \u0026#39;3\u0026#39; services: vikunja: image: vikunja/vikunja environment: VIKUNJA_SERVICE_PUBLICURL: https://vikunja.jimsgarage.co.uk VIKUNJA_DATABASE_HOST: db:3306 VIKUNJA_DATABASE_PASSWORD: changeme VIKUNJA_DATABASE_TYPE: mysql VIKUNJA_DATABASE_USER: vikunja VIKUNJA_DATABASE_DATABASE: vikunja VIKUNJA_SERVICE_JWTSECRET: dskfj23kkjsdkjsdfjkl3sdfgfsdfhsdfkh3ozxpqzx volumes: - ./files:/app/vikunja/files networks: - proxy depends_on: db: condition: service_healthy restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.vikunja.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.vikunja.rule=Host(`vikunja.jimsgarage.co.uk`)\u0026#34; - \u0026#34;traefik.http.middlewares.vikunja-https-redirect.redirectscheme.scheme=https\u0026#34; - \u0026#34;traefik.http.routers.vikunja.middlewares=vikunja-https-redirect\u0026#34; - \u0026#34;traefik.http.routers.vikunja-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.vikunja-secure.rule=Host(`vikunja.jimsgarage.co.uk`)\u0026#34; - \u0026#34;traefik.http.routers.vikunja-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.vikunja-secure.service=vikunja\u0026#34; - \u0026#34;traefik.http.routers.vikunja.tls.certResolver=cloudflare\u0026#34; - \u0026#34;traefik.http.services.vikunja.loadbalancer.server.port=3456\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; db: image: mariadb:10 command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: supersupersecret MYSQL_USER: vikunja MYSQL_PASSWORD: changeme MYSQL_DATABASE: vikunja networks: - proxy volumes: - ./db:/var/lib/mysql restart: unless-stopped healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;mysqladmin ping -h localhost -u vikunja --password=changeme\u0026#34;] interval: 2s start_period: 30s networks: proxy: external: true ","permalink":"https://deuts.org/p/vikunja-update-2024/","summary":"There seems to be a new update on Vikunja.","title":"Update on Vikunja"},{"content":"Insider Business shows 11 of the most faked foods in the world, which include:\nTruffles Maple Syrup Wasabi Parmesan Cheese Vanilla Caviar Honey Olive Oil Wagyu Beef Coffee Saffron \u0026nbsp; Of the 11, I would say fake parmesan cheese, honey and olive oil are the most I probably am guilty of consuming. I realize this now, I should be cautious when I\u0026rsquo;m doing my groceries next time.\n","permalink":"https://deuts.org/p/most-faked-food/","summary":"We need to be more vigilant the next time we do our groceries.","title":"11 Most Faked Foods In The World"},{"content":"I\u0026rsquo;ve recently seen these Youtube shorts that promote The Smoothie Diet. I\u0026rsquo;m sure the Youtube channel that uploads these videos (@Animememes1) is simply grabbing some weight transformation videos from Tiktok, upload them as Shorts in their Youtube channel, and adding a link in the comments about the Smoothie Diet.\nPlease don\u0026rsquo;t fall for this scam. These smoothies are so full of sugar, you don\u0026rsquo;t want them on your bodies. The real weight-loss strategy is totally free — you don\u0026rsquo;t have to pay a dime, you have nothing in advance to prepare: Intermittent Fasting.\nLook it up, especially about the power and benefits of the \u0026lsquo;clean\u0026rsquo; fast. And while you\u0026rsquo;re at it, you would help yourself better if you try and read Gin Stephen\u0026rsquo;s Fast, Feast, Repeat and Delay, Don\u0026rsquo;t Deny.\n","permalink":"https://deuts.org/p/the-smoothie-diet-scam/","summary":"Please, don\u0026rsquo;t fall for this scam. The real eating habit that\u0026rsquo;ll totally bring weight-loss and health benefits is Intermittent Fasting. Trust me.","title":"The Smoothie Diet Scam"},{"content":"I would really like to learn Python to reinforce my data analysis skills. And I would like to start with this 4-hour free course on Youtube.\nSharing this here if you want to join me as well.\n\u0026nbsp; ","permalink":"https://deuts.org/p/data-analysis-python-excel/","summary":"Let\u0026rsquo;s start learning Python with this 4-hour free course on Youtube.","title":"Data Analysis with Python for Excel Users"},{"content":"Vikunja is a powerful open-source todo app that provides users with a range of features designed to enhance productivity. It offers task tracking, due date management, collaboration tools, and more, all within an intuitive interface.\nInstalling via Vikunja via Docker, though, requires quite a number of containers running. The normal install alone needs a minimum 4 containers:\ndatabase api frontend nginx When I tried running them via docker-compose, I encountered a lot of this kind of Internal Server Error:\nAnd sometimes logging in returned repetitive API errors.\nI read somewhere that installing Redis along within the same docker-compose file helps. So I did.\nBut I still encountered several Internal Server Errors, albeit on rarer occasions this time.\n5 Docker Containers, and still errors. Vikunja left me no choice but to run:\ndocker-compose down docker image prune -a cd .. rm -R vikunja ","permalink":"https://deuts.org/p/vikunja-internal-server-error/","summary":"Even with all the fancy containers, I find it not reliable.","title":"I can't make Vikunja work on my set up"},{"content":"After installing Nextcloud via Docker, and if you go to /settings/admin/overview, you might find some warnings that you need to do further configurations to get rid of.\nService Discovery Your web server is not properly set up to resolve \u0026ldquo;/.well-known/caldav\u0026rdquo;. Your web server is not properly set up to resolve \u0026ldquo;/.well-known/carddav\u0026rdquo;.\nGo to the app/ directory where you\u0026rsquo;ll find the .htaccess file, and change the following lines:\nRewriteRule ^\\.well-known/carddav /remote.php/dav/ [R=301,L] RewriteRule ^\\.well-known/caldav /remote.php/dav/ [R=301,L] To:\nRewriteRule ^\\.well-known/carddav https://%{SERVER_NAME}/remote.php/dav/ [R=301,L] RewriteRule ^\\.well-known/caldav https://%{SERVER_NAME}/remote.php/dav/ [R=301,L] Source Cron Jobs not working You will need to setup a cron job from your host system. Try:\nsudo crontab -e Then add this line at the bottom:\n*/5 * * * * docker exec -u www-data [containername] php /var/www/html/cron.php Make sure to change [containername] to the actual name of your container.\nSource ","permalink":"https://deuts.org/p/nextcloud-post-install-config/","summary":"Get rid of several warnings in your Nextcloud admin dashboard.","title":"A few Nextcloud post-installation configurations"},{"content":"Update the system From this moment, we\u0026rsquo;ll assume you\u0026rsquo;re still logged in as root. After all, this is a fresh install of Linux, right?\napt update apt upgrade -y Install essential utilities apt install sudo htop curl nano wget net-tools Change root password This is just in case you don\u0026rsquo;t think the assigned root password is not complex enough for your liking:\npasswd Then enter you new password twice.\nUpdate Timezone dpkg-reconfigure tzdata Add non-root user Add user Change username with the username of your choice:\nadduser username Add user to sudoers group usermod -aG sudo username So, the non-root user is ready. From this moment on, we\u0026rsquo;ll assume you\u0026rsquo;re logged as that non-root user.\nChange the hostname Edit /etc/hostname sudo nano /etc/hostname And change accordingly.\nEdit /etc/hosts sudo nano /etc/hosts And change accordingly.\nReboot sudo reboot Install Docker and Docker Compose Install Docker Follow the instructions applicable for your system from the official Docker documentation. My favorite systems are Ubuntu and Debian, and I actually prefer to use the install using the repository method:\nSet up Docker\u0026rsquo;s Apt repository For Ubuntu # Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update For Debian # Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update Install the Docker packages sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Install Docker Compose sudo apt install docker-compose Add user to the Docker group Check if docker group already exists: grep docker /etc/group Add user to the docker group: usermod -aG docker username (don\u0026rsquo;t forget to change username to the username of your choice) Create a network bridge for Docker Containers From now on, the instructions here is if you plan to expose your applications via Cloudflare Tunnel.\ndocker network create -d bridge cloudflared Install Cloudflare Tunnel Make a directory: mkdir appdata/cloudflared (the directory structure is up to you) CD to that directory: cd appdata/cloudflared Create the docker-compose.yml file: nano docker-compose.yml Then paste the following:\nversion: \u0026#34;3.3\u0026#34; services: tunnel: container_name: cloudflared-tunnel image: cloudflare/cloudflared restart: unless-stopped command: tunnel --no-autoupdate run environment: - TUNNEL_TOKEN=[paste_here_your_actual_token] networks: - cloudflared networks: cloudflared: external: true Don\u0026rsquo;t forget to paste your actual token that will be generated when you create a new tunnel in Cloudflare.\nInstall Portainer Via Docker Compose Same as above, mkdir for portainer, then create the docker-compose.yml file:\nversion: \u0026#39;3\u0026#39; services: portainer: image: portainer/portainer-ce:latest container_name: portainer environment: - PUID=1000 - PGID=1000 - TZ=Asia/Manila restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock - ./data:/data # ports: # - 9000:9000 # - 8000:8000 # - 9443:9443 networks: - cloudflared networks: cloudflared: external: true Note that I commented out the ports portion of the docker-compose.yml because we\u0026rsquo;re not exposing the ports to the internet. Instead, we\u0026rsquo;re using Cloudflare Tunnel to expose the apps.\nInstall Duplicati All these efforts you do to set up Docker containers will go to waste if you don\u0026rsquo;t do a proper backup of your config and data files.\nversion: \u0026#34;2.1\u0026#34; services: duplicati: image: lscr.io/linuxserver/duplicati:latest container_name: duplicati environment: - PUID=0 - PGID=0 - TZ=America/Denver volumes: - ./config:/config - ./backups:/backups - /home/username:/source # ports: # - 8200:8200 restart: unless-stopped networks: - cloudflared networks: cloudflared: external: true I prefer to run Duplicati as root so I won\u0026rsquo;t have to deal with read permission issues in the future, so I set PUID and PGID to 0. Needless to say, you have to change your TZ and your source folder to map to the container.\n","permalink":"https://deuts.org/p/new-vps-checklist/","summary":"Got a new Ubuntu or Debian VPS server? Here are the things you should do to make the most out of it.","title":"Checklist of things to do on a new VPS server"},{"content":"Via APT If you\u0026rsquo;re running Debian or Ubuntu on your VPS, you can actually easily install Hugo via:\nsudo apt install hugo However, the version you can get from the repository is so old. Mine\u0026rsquo;s around version 0.80 I think.\nVia Deb file from Github Download and Run If you want the latest version of Hugo installed, you need to get it from github. As of this writing, the latest version is v0.117.0. Thus, you can run:\nwget https://github.com/gohugoio/hugo/releases/download/v0.117.0/hugo_extended_0.117.0_linux-amd64.deb sudo dpkg -i hugo_extended_0.117.0_linux-amd64.deb Copy the Hugo file By default, using the above method, hugo is saved under /usr/local/bin. You can verify that by running which hugo command. But this needs to be copied to the /usr/bin folder. Thus, run:\nsudo cp /usr/local/bin/hugo /usr/bin/ Check version You can double check if indeed you have the latest version by running:\nhugo version Updating Hugo to the latest release Redo all the installation instructions above including the sudo cp /usr/local/bin/hugo /usr/bin/ command.\n","permalink":"https://deuts.org/p/hugo-linux-vps/","summary":"Let me help you run Hugo on a Debian or Ubuntu Linux server.","title":"How to install Hugo in a Linux VPS"},{"content":"Quickly The easy approach is to use:\ndocker ps --size But this will output a long table with columns:\nContainer ID Image Command Created Status Ports Names Size Let\u0026rsquo;s refine the command More likely, you\u0026rsquo;re just curious about these columns:\nContainer ID Names Image Size In order to do so, just run:\ndocker ps -a --size --format \u0026#34;table {{.ID}}\\t{{.Names}}\\t{{.Image}}\\t{{.Size}}\u0026#34; Source ","permalink":"https://deuts.org/p/docker-container-size/","summary":"Which image occupies my VPS storage the most?","title":"Docker Command to Check the Sizes of your Containers"},{"content":"Have you ever encountered issues with legacy keyrings when running a fresh install of Debian or Ubuntu?\nW: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details\nThe Quick Fix cd /etc/apt sudo cp trusted.gpg trusted.gpg.d sudo apt update More info can be read here.\n","permalink":"https://deuts.org/p/key-legacy-trusted-gpg-keyring/","summary":"How to get rid of this deprecation error in Ubuntu or Debian?","title":"Key Is Stored in Legacy Trusted.gpg Keyring"},{"content":"What if:\nYou installed Nextcloud via Docker You created files or subfolders in the Nextcloud folder of a user This is also true if you\u0026rsquo;re restoring files and folders from a backup You can\u0026rsquo;t see the files or subfolders in the Nextcloud web interface Just run the following command, replacing nextcloud_app with the actual container name of your Nextcloud Docker install:\ndocker exec -u www-data nextcloud_app php occ files:scan --all This is working for me, on Nextcloud version 27.0.1.\n","permalink":"https://deuts.org/p/how-to-rescan-docker-nextcloud-folders/","summary":"Wait, where did my files go?","title":"How to Rescan Your Docker Nextcloud Folders"},{"content":"This article promises I would enjoy Andor. I should probably start watching these Star Wars TV shows.\nHow come every time I come here to just post a quick short post, I end up with a long one?\nExcel is dead? What a stupid question. This guy doesn\u0026rsquo;t use Microsoft Excel as much as I do. Such a click-bait.\nYugatech: Disney+ coming to the Philippines on November 17. Price starts at P159 for mobile plan and P369 for premium. Take your pick: you split the subscription price, or just do everything, every streaming service, on Plex.\nA bridge collapsed in Bayambang, Pangasinan. That\u0026rsquo;s why it\u0026rsquo;s so scary to be stuck in traffic while in the middle of a bridge.\nThe Underrated TV Shows in 2022 I think these are the shows you shouldn\u0026rsquo;t miss in 2022:\nWeCrashed Slow Horse The Dropout Severance Winning Time: The Rise of the Lakers Dynasty Blackbird Of course, the House of the Dragon is not mentioned in this list, because it belongs to a totally different list.\nLegacy: The True Story of the LA Lakers This TV series is so full of rich stories about the story behind the LA Lakers upon the ascension of Jerry Buss to the head of the organization. But it\u0026rsquo;s also full of interviews, and less drama actually.\n","permalink":"https://deuts.org/p/underrated-tv-shows-2022-et-al/","summary":"Start watching them the soonest you can!","title":"The Underrated TV Shows of 2022 Et Al"},{"content":"There are times that you you want to use the Table.Skip Power Query function, but the number of rows you want to skip varies from file to file in your folder. You want the number of rows to be dynamic.\nThe function below will count the number of rows under the column Column4 while the rows are nulls.\nTable.Skip(Source, each [Column4] = null) Watch the video.\n","permalink":"https://deuts.org/p/power-query-table-skip-while-row-is-blank/","summary":"Use the Table.Skip function the dynamic way.","title":"Power Query Table.Skip While Row Is Blank"},{"content":"With all the hype about ChatGPT nowadays, I really have to try it. And I\u0026rsquo;m still wondering how to install Mastodon on Debian 11 without Docker. And here\u0026rsquo;s the AI\u0026rsquo;s response:\nHow to Install Mastodon on Debian 11 Without Docker (instructions generated from ChatGPT) To install Mastodon, the open-source, decentralized social networking platform, on Debian 11 without using Docker, you can follow these steps:\nFirst, make sure you have the necessary dependencies installed on your system. This includes PostgreSQL, Redis, and Yarn. You can install these using the following commands: sudo apt-get update sudo apt-get install postgresql postgresql-contrib sudo apt-get install redis-server curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install yarn Next, clone the Mastodon Git repository to your local machine using the following command: git clone https://github.com/tootsuite/mastodon.git Navigate to the Mastodon directory and install the necessary dependencies using Yarn: cd mastodon yarn install Next, you will need to configure Mastodon by copying the .env.production.sample file to .env.production and editing it to set the appropriate values for your environment. This includes setting up your database, caching, and other options.\nOnce the configuration is complete, you can compile the Mastodon assets using the following command:\nRAILS_ENV=production bundle exec rails assets:precompile Finally, you can start the Mastodon server using the following command: RAILS_ENV=production bundle exec rails s Mastodon should now be running on your system. You can access it in your web browser at http://localhost:3000.\nNote: The steps outlined above are a simplified version of the installation instructions for Mastodon. For more detailed instructions and additional options, please refer to the Mastodon documentation.\nI still have to try this And I still have to try this soon. Will provide feedback if this instuction prove helpful.\n","permalink":"https://deuts.org/p/install-mastodon-debian-11-without-docker/","summary":"This is how ChatGPT would install Mastodon on a bare metal VPS.","title":"How to Install Mastodon on Debian 11 Without Docker"},{"content":"Abrar Al-Heeti over at CNET discusses why she\u0026rsquo;s now Team Apple and never going back.\nIf I were to summarize her reasoning for doing so, the major reason is iMessage and Airdrop. The rest are just aesthetics.\niMessage I don\u0026rsquo;t have an iPhone right now, but who texts nowadays anyway? If you look closely at my messages app, they\u0026rsquo;re mostly made up of OTPs and marketing messages. Not much from real contacts.\nWhere I\u0026rsquo;m from and where I work, messaging is usually done via Viber and Facebook Messenger. So I\u0026rsquo;ll just say damn to those who insist on iMessage.\nMoreover, I think the Android messages app has a more robust anti-spam capability than the iPhone. By the way, did you know that you can\u0026rsquo;t change the default SMS app in the iPhone?\nAirdrop Who needs original quality photos? They\u0026rsquo;re big in file size and you\u0026rsquo;re not printing them into billboards anyway. And if you really need it, there are alternatives other than Google Drive.\nIndispensable on my Android Dual Sim Yes, there are dual-sim and e-Sim capable iPhones but you have to be very special to use one especially in the Philippines. Globe and Smart can only activate your e-Sim if you\u0026rsquo;re on a postpaid plan. You\u0026rsquo;ll need to procure yours from HongKong if you want a dual-sim iPhone. Almost every Android phone has dual-sim capability.\nTorrenting apps I don\u0026rsquo;t use my uTorrent app too often on my Android phone as I have a Qbittorrent hosted on a VPS, but I\u0026rsquo;m glad that it\u0026rsquo;s there when I need it.\nSyncthing Not on an iPhone. Period.\nUSB Type-C This may change soon, but I\u0026rsquo;m leaving it here anyway. I can carry around with me just one charger for my laptop, tablet and smartphone.\nCustomizable Home Screen This feature may become a time-waster sometimes, even a resource-hog, but the fact that I can move around app icons even to the bottom of the screen offers a lot of flexibility on how I want my phone to look each time I unlock it.\nIf you\u0026rsquo;re not too convinced, check out r/NovaLauncher and r/NiagaraLauncher for inspiration.\nExpandable Storage This feature probably becomes less attractive as built-in storage options become larger, but I\u0026rsquo;m still attracted to this option, and plays a role in my decision to stick to Android, at least for the foreseeable future.\nDual Accounts/Profile I know apps like Facebook and Messenger have the capability to run multiple profiles in the same app, but I don\u0026rsquo;t think you can get notifications for them all.\nWith features like in the Samsung\u0026rsquo;s secret folder, you can essentially have two separate Viber accounts for example running, and you\u0026rsquo;ll get notifications from both accounts.\n","permalink":"https://deuts.org/p/iphone-vs-android/","summary":"One of those posts explaining why an Android device is better than the iPhone.","title":"Yet another iPhone vs. Android post"},{"content":" Bitcoin isn\u0026rsquo;t really used much as a currency at all.\nWhen people buy bitcoin, they rarely use it to buy goods and services. They\u0026rsquo;re really buying access to a Ponzi scheme.\n\u0026nbsp; I subscribe to this school of thought. Bitcoin is probably a good idea as a currency. Problem is, people don\u0026rsquo;t exchange their dollars for Bitcoin and use it to buy goods and services. Instead, they buy it in the hopes that other people will hop on the bandwagon, and cash-in on its price appreciation.\nThe same thing applies to other Cryptocurrencies.\n","permalink":"https://deuts.org/p/why-bitcoin-is-a-scam/","summary":"No intrinsic value, it\u0026rsquo;s a scam!","title":"Why Bitcoin Is a Scam"},{"content":"I was wondering why Docker apps don\u0026rsquo;t come as easy to install like Linuxserver\u0026rsquo;s? Case in point: Joplin.\nWell, I really do think Joplin server\u0026rsquo;s documentation needs a lot more polishing. Fortunately I was able to make it work in my system, even without exposing additional ports to the public, which I\u0026rsquo;m sharing below.\nWhy Joplin Server instead of Webdav, Dropbox, OneDrive, NextCloud, etc. According to Noted:\nSpeed up the sync\nI still have to find out if this really is true Sharing a note with anyone, using a URL\nThis is actually my favorite feature. This enables me to share notes that are not so private to other people by just sharing a link to the public page. Then as I update my note, the page also updates. User access\nI still have to evangelize other people about Joplin so I could add users to my Joplin server, if ever they need a sync server. Sharing a notebook with a user on the same Joplin Server\nThis is also a great feature, especially if I have other users I can share my Joplin server with. Docker Compose for Joplin Server Docker Network Make sure that you already have a ready Docker Network for your Joplin app and database. This will make sure that you don\u0026rsquo;t expose additional ports to the public. If you don\u0026rsquo;t have a dedicated network yet, just run:\nsudo docker network create -d bridge examplenetwork Nginx Proxy Manager I would assume that you already have the Nginx Proxy Manager installed and it\u0026rsquo;s running in the same examplenetwork . Now, add a new Proxy Host with your domain name, e.g. joplin.example.com, then hostname should be joplin_app (container name below) and port 22300.\nInstall Joplin App and DB On your favorite directory, just create the docker-compose.yml file, which should contain:\nversion: \u0026#39;3\u0026#39; services: db: image: postgres:13 container_name: joplin_db volumes: - ./data/postgres:/var/lib/postgresql/data # ports:\twe don\u0026#39;t need this anymore because we\u0026#39;re reverse proxying anyway # - \u0026#34;5432:5432\u0026#34; restart: unless-stopped environment: - POSTGRES_PASSWORD=yHZ4TbsyKJI0Xi2sUmXDuz - POSTGRES_USER=Barrier1542 - POSTGRES_DB=Sudden9997 networks: - examplenetwork app: image: joplin/server:latest depends_on: - db container_name: joplin_app # ports:\twe don\u0026#39;t need this anymore because we\u0026#39;re reverse proxying anyway # - \u0026#34;22300:22300\u0026#34; restart: unless-stopped environment: - APP_PORT=22300 - APP_BASE_URL=https://joplin.example.com - DB_CLIENT=pg - POSTGRES_PASSWORD=yHZ4TbsyKJI0Xi2sUmXDuz - POSTGRES_DATABASE=Sudden9997 - POSTGRES_USER=Barrier1542 - POSTGRES_PORT=5432 - POSTGRES_HOST=db networks: - examplenetwork networks: examplenetwork: external: true Don\u0026rsquo;t worry about the usernames and passwords in my sample docker-compose.yml file, they were just randomly generated and not used for production elsewhere.\nThen run sudo docker-compose up -d\nDid you know that the Joplin docker image alone is worth 1.2GB of storage in your server? Postgres is another 373MB.\nBackup with Duplicati As Joplin populates your /data/postgres folder in the initial setup, as well as along the way as you actually use your server, it creates files and directories with user and group permissions assigned to systemd-coredump. Worse, they are readable and writable by the user only. That is usually fine, until you try backing them up using Duplicati.\nThe solution: run your Duplicati docker app with root privileges.\nI know the rule about not running apps as root. But that\u0026rsquo;s the only way I see so far that work. Perhaps, Joplin could give us an option to create and update files in the persistent volumes as a regular user. That way, I can run Duplicati as that same user and could access the files for backup.\nUpdate as of Mar. 24, 2024 For whatever reason, recently I noticed my Joplin server was down. The GUI is not accessible, and the sync process is not pushing though. Looked at the log, and it appears the Joplin app container is having trouble connecting to the Postgres database.\nIn order to fix it, I needed to remove all the reference to the examplenetwork and uncomment all ports to make them active. Moreover, I needed to point directly the hostname in Nginx Proxy Manager (NPM) to the server\u0026rsquo;s IP address.\nversion: \u0026#39;3\u0026#39; services: db: image: postgres:13 container_name: joplin_db volumes: - ./data/postgres:/var/lib/postgresql/data ports: - \u0026#34;5432:5432\u0026#34; restart: unless-stopped environment: - POSTGRES_PASSWORD=yHZ4TbsyKJI0Xi2sUmXDuz - POSTGRES_USER=Barrier1542 - POSTGRES_DB=Sudden9997 app: image: joplin/server:latest depends_on: - db container_name: joplin_app ports: - \u0026#34;22300:22300\u0026#34; restart: unless-stopped environment: - APP_PORT=22300 - APP_BASE_URL=https://joplin.example.com - DB_CLIENT=pg - POSTGRES_PASSWORD=yHZ4TbsyKJI0Xi2sUmXDuz - POSTGRES_DATABASE=Sudden9997 - POSTGRES_USER=Barrier1542 - POSTGRES_PORT=5432 - POSTGRES_HOST=db ","permalink":"https://deuts.org/p/joplin-npm-duplicati-docker/","summary":"The ultimate way to install the Joplin server while using NPM and Docker.","title":"Make Joplin work with Nginx Proxy Manager and Duplicati (Docker)"},{"content":"When I started hoarding movies and TV shows back when streaming services were not yet a thing, of course I played them using VLC. But you can only do that on a computer. These media are better consumed over TV, right?\nThen, the rise of Netflix showed us a glipmse of how your media should be better managed — with posters or thumbnails, description or summary, genre, casts list, and even related movies to watch after. I remember I used to have several hard drives with duplicate movies from several sources or several different versions and video codecs. Worse, it was so hard to keep track back then which movies you\u0026rsquo;ve already watched or what episode of a show did you last watch.\nThen came Kodi. And you can sync your collection and watch status with Trakt. But Kodi was supposed to manage your local files only — or so I knew.\nIf you have a VPS or even a local server with your media files while running Apache or Nginx to serve webpages and/or files, you can actually add those folders as a media source to your Kodi app. The video below will show you how:\n\u0026nbsp; Of course, now there are media servers like Plex, Emby and Jellyfin to help you manage and stream your media files. But Kodi has a lot to offer over these other apps, and that\u0026rsquo;s a totally different topic.\nFAQ Will this work with HTTP Basic Authentication on Nginx server? I actually have yet to try this out, and will update this post when I find out. But feel free to mention me @deuts if you know the answer.\n","permalink":"https://deuts.org/p/streaming-movies-tv-shows-kodi/","summary":"This may not work currently on my setup, but leaving it here anyway.","title":"Streaming Movies and TV Shows on Kodi"},{"content":"These are the TV shows that I deleted from my Sonarr installation, but may consider following or monitoring them in the future.\nBad Sisters Gaslit Hacks Mindhunter The Morning Show Only Murders in the Building Season 2 Severance Season 2 Slow Horses Season 2 The Time Traveler\u0026rsquo;s Wife Warrior What We Do in the Shadows Several of these I have watched the Season 1 already, and will definitely add them back when Season 2 are ready.\n","permalink":"https://deuts.org/p/tv-shows-consider-watching-future/","summary":"The must see shows in the future according to DeutsFlix.","title":"TV Shows I May Consider Watching in the Future"},{"content":"Ran my 3 websites on GT Metrix, and this is what I got:\nCurrent, they are running on the following platforms:\nWebsite Platform deuts.org Hugo deuts.net WordPress chesshive.com WordPress That goes to show how efficient static site generators like Hugo can be to make your websites more user and search friendly.\n","permalink":"https://deuts.org/p/gt-metrix-comparison-3-websites/","summary":"A Hugo site, being all just HTML files, can really be that fast.","title":"GT Metrix Comparison of my 3 Websites"},{"content":" \u0026nbsp; I don\u0026rsquo;t think they are comparable. Notion is for note-taking app/service while Airtable is for databases and workflow. You may also build databases in Notion but it\u0026rsquo;s quite limited in functionality. And Airtable has robust API that you can interact with using Excel.\n","permalink":"https://deuts.org/p/notion-vs-airtable/","summary":"Airtable for the win!","title":"Notion vs Airtable"},{"content":"Bumped into this post on Reddit, asking if his \u0026lsquo;old\u0026rsquo; laptop can handle Plex:\nImagine: a 7th-gen i5 with a dedicated NVIDIA graphics, and he\u0026rsquo;s asking if his laptop can handle Plex. I\u0026rsquo;m sure this spec can handle 1 or 2 4K streams easily.\nI have an old Beelink S1 Mini PC, with an Intel Celeron N3450, Quad Core, 4GB of RAM and 64GB eMMC, and this thing handles Plex well, albeit I only watch 720p movies and shows. But it\u0026rsquo;s running not just Plex, but a host of other docker containers, including but not limited to:\nRadarr Sonarr Nginx Proxy Manager WordPress server and database Tautulli Portainer Syncthing Duplicati Jackett NZBGet Dockuwiki ruTorrent So, yeah, to answer OP\u0026rsquo;s question, it is very much capable.\n","permalink":"https://deuts.org/p/can-old-laptop-handle-plex/","summary":"An Intel 7th Gen i5 is more than enough to run Plex.","title":"Can This Old Laptop Handle Plex?"},{"content":"Long commands are hard to memorize Let\u0026rsquo;s say for example you want to run hugo server, but instead of on the localhost you want to run it over your VPS server (or even from your home server). Without the bash script, you\u0026rsquo;ll need to run:\nhugo server --bind=\u0026lt;IP ADDRESS\u0026gt; --baseURL=http://\u0026lt;IP ADDRESS\u0026gt;:1313 Create a bash script Create the bash file Let\u0026rsquo;s name the bash file serve.sh\nnano serve.sh Enter your commands #!/bin/bash hugo server --bind=\u0026lt;IP ADDRESS\u0026gt; --baseURL=http://\u0026lt;IP ADDRESS\u0026gt;:1313 Don\u0026rsquo;t forget to include #!/bin/bash at the first line.\nMultiple Commands If you need to make multiple commands, enter each command in separate lines, and end the file with exec bash, like for example:\n#!/bin/bash cd appdata/app/subfolder PS1=\u0026#39;$(whoami)@$(hostname):$(pwd)# \u0026#39; exec bash Ctrl+O to save the file\nCtrl+X to close the file\nMake it executable sudo chmod +x serve.sh Enter password as may be necessary.\nRun the script ./serve.sh Assuming we don\u0026rsquo;t get into permission problems, our script should be working well.\n","permalink":"https://deuts.org/p/create-bash-script-executable/","summary":"Put long commands in a bash script so you don\u0026rsquo;t have to remember them.","title":"How to Create a Bash Script and Make it Executable?"},{"content":" Ano ang nasa dakong paroon,\nbunga ng malilikot na pagiisip,\nlikha ng balintataw o halaw\nmula sa daigdig ng kababalaghan\ndi kayang ipaliwanag\nngunit alam mong magaganap\naawooooo!\n","permalink":"https://deuts.org/p/ano-ang-nasa-dakong-paroon/","summary":"Bunga ng malilikot na pag-iisip.","title":"Ano Ang Nasa Dakong Paroon"},{"content":"Price Increase Announcement Got an email today about the forthcoming price increase from Contabo. Also saw the discussion on Lowendtalk.\nWhat I use it for? Currently, I have my Plex server, Radarr, Sonarr, Qbittorrent, Bazarr, Tautulli, among others, installed on my Contabo servers. Beginning November 1, it appears my fee will increase from EUR4.99 to EUR5.99 per month. I know it\u0026rsquo;s not too much, especially for a 400GB SSD storage, 4-core CPU and 8GB RAM VPS server. It\u0026rsquo;s pretty much like paying for a Netflix subscription, but with so much more flexibility.\nBlack Friday Sale Black Friday is fast approaching. I\u0026rsquo;ll check out the Lowendbox and Lowendtalk sites for attractive sale that may fit my needs. Until then, I\u0026rsquo;ll keep my Contabo server.\n","permalink":"https://deuts.org/p/contabo-price-increase-2022/","summary":"It\u0026rsquo;s not worth it, I promise you!","title":"Contabo Price Increase 2022"},{"content":"This video runs you through the interface of basketball video games way back to 1974. To me, it brings back not really a lot of memories, but particular memories, like the Double Dribble of the Family Computer days and the NBA Live 2004 of office tournaments via LAN days.\n\u0026nbsp; ","permalink":"https://deuts.org/p/basketball-video-games-evolution/","summary":"Look at how the graphics improved over the years.","title":"Basketball Video Games Evolution"},{"content":"Don\u0026rsquo;t you find this outlook from BPI a little too agressive for your taste? And they call it \u0026lsquo;Base Case Scenario\u0026rsquo;.\nUSD/PHP Quarterly Outlook 4Q-2022 to FY 2023 Forecast (end of period) Base Scenario 4Q 2022 58.100 1Q 2023 58.705 2Q 2023 59.311 3Q 2023 59.916 4Q 2023 60.521 ","permalink":"https://deuts.org/p/bpi-usd-php-outlook-2023/","summary":"The unattainable forecasts.","title":"BPI USD/PHP Outlook 2023"},{"content":"Here\u0026rsquo;s a detailed tutorial on how to setup and run Oracle Cloud Free Tier.\n\u0026nbsp; Ain\u0026rsquo;t this too good to be true. Two things you have to be wary about:\nIt\u0026rsquo;s from Oracle, depends on you if you trust that name. But I worry how long will it remain free. It\u0026rsquo;s running on ARM, so your favorite docker app might not be compatible. Out of Capacity So I tried sigining up for Oracle Free Tier VM instance, and of course I bumped into the dreaded \u0026lsquo;Out of capacity for shape \u0026hellip; in availability domain \u0026hellip;\u0026rsquo;\nIndeed, too good to be true!\n","permalink":"https://deuts.org/p/oracle-cloud-free-tier/","summary":"A guide to using Oracle Cloud Free Tier—just a heads-up: Oracle’s reliability can be hit-or-miss, Docker may have ARM compatibility issues, and I encountered an \u0026ldquo;Out of capacity\u0026rdquo; error at signup.","title":"Oracle Cloud Free Tier"},{"content":" \u0026nbsp; This is exactly what brave men should say:\nIf you tell me I\u0026rsquo;m wrong\nI don\u0026rsquo;t wanna be right\n","permalink":"https://deuts.org/p/nico-vinz-am-i-wrong/","summary":"Dedicated to all the brave men out there.","title":"Nico \u0026 Vinz - Am I Wrong"},{"content":"I wonder how I\u0026rsquo;ve come to know this function just now. I have so much use for the STOCKHISTORY function to manage my personal stock portfolio.\n\u0026nbsp; ","permalink":"https://deuts.org/p/excel-stockhistory-function-pro-tips/","summary":"The first time I learned about \u0026lsquo;STOCKHISTORY\u0026rsquo; function.","title":"Excel Stockhistory Function Pro Tips"},{"content":"I really want to try this at home. They\u0026rsquo;re so expensive to order outside, now I want unli beef recipes on my table.\n\u0026nbsp; Recipes include:\nGarlic Pepper Beef (Filipino) Bulgogi (Korean) Pepper Beef (ala Pepper Lunch) (Japanese) What are the other better Bulgogi ingredients I can try?\n","permalink":"https://deuts.org/p/beef-fast-food-recipes/","summary":"Get ready to grill! Your kitchen\u0026rsquo;s about to go full Bulgogi bonanza—hope you\u0026rsquo;re prepared to raise the steaks on flavor!","title":"Beef Fast Food Recipes"},{"content":" History does not remember blood.\nIt remembers names.\n— Corlys Velaryon\n","permalink":"https://deuts.org/p/blood-vs-name/","summary":"A powerful reminder that what we leave behind is often more about our legacy and reputation than the battles we\u0026rsquo;ve fought.","title":"Blood vs. Name"}]